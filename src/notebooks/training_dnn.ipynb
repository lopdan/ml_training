{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises from _Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Is it okay to initialize all the weights to the same value as long as that value is\n",
    "selected randomly using He initialization?**\n",
    "\n",
    "Since we want to avoid symmetries by breaking them, initializing all values to the same will make \n",
    "all the weights to be the same, thus, making impossible to break the symmetry.\n",
    "\n",
    "**2. Is it okay to initialize the bias terms to 0?**\n",
    "\n",
    "Yes, it is okay, it doest not make much difference.\n",
    "\n",
    "**3. Name three advantages of the SELU activation function over ReLU.**\n",
    "\n",
    "It can take negative values, alliviating the vanishing gradients problem.\n",
    "It has a nonzero derivative, avoiding dying units.\n",
    "It is smooth everywhere, since ReLU jumps from 0 to 1 at given point.\n",
    "\n",
    "**4. In which cases would you want to use each of the following activation functions:\n",
    "ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "\n",
    "ELU, leaky ReLU: If you need the neural network to be as fast as possible.\n",
    "\n",
    "ReLU: Autoencoders.\n",
    "\n",
    "tanh: In a output layer if a number between -1 and 1 is needed.\n",
    "\n",
    "logistic: In the output layer to estimate a probability.\n",
    "\n",
    "softmax: In the output layer for probabilities that are mutually exclusive classes.\n",
    "\n",
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g.,\n",
    "0.99999) when using an SGD optimizer?**\n",
    "\n",
    "The algorithm will pick up a lot of speed and it will shoot right past the minimum. It will\n",
    "make it several times before converging, resulting in a slower training time.\n",
    "\n",
    "**6. Name three ways you can produce a sparse model.**\n",
    "\n",
    "Zero out tiny weights. Also apply l1 regularization during training, making it more sparse. At last combining\n",
    "l1 regularization with dual averaging.\n",
    "\n",
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making\n",
    "predictions on new instances)? What are about MC dropout?**\n",
    "\n",
    "Yes, dropout does slow down training, in general roughly by a factor of two.\n",
    "However, it has no impact on inference since it is only turned on during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Adam optimization and early stopping, try training it on MNIST but\n",
    "# only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\n",
    "# next exercise. You will need a softmax output layer with five neurons, and as\n",
    "# always make sure to save checkpoints at regular intervals and save the final\n",
    "# model so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data(path='mnist.npz')\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and checkpoint\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\", save_best_only=True)\n",
    "\n",
    "run_index = 1 \n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 4.6982 - accuracy: 0.3411 - val_loss: 1.6039 - val_accuracy: 0.4944\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 1.1802 - accuracy: 0.6219 - val_loss: 0.8530 - val_accuracy: 0.7242\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7334 - accuracy: 0.7645 - val_loss: 0.5988 - val_accuracy: 0.8116\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5227 - accuracy: 0.8367 - val_loss: 0.5052 - val_accuracy: 0.8418\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.4035 - accuracy: 0.8768 - val_loss: 0.3523 - val_accuracy: 0.8966\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.3282 - accuracy: 0.8993 - val_loss: 0.3161 - val_accuracy: 0.9072\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2734 - accuracy: 0.9183 - val_loss: 0.2885 - val_accuracy: 0.9204\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2346 - accuracy: 0.9287 - val_loss: 0.2663 - val_accuracy: 0.9228\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2049 - accuracy: 0.9387 - val_loss: 0.2352 - val_accuracy: 0.9314\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1789 - accuracy: 0.9454 - val_loss: 0.2249 - val_accuracy: 0.9382\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1583 - accuracy: 0.9526 - val_loss: 0.2110 - val_accuracy: 0.9370\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1435 - accuracy: 0.9571 - val_loss: 0.2122 - val_accuracy: 0.9420\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1292 - accuracy: 0.9608 - val_loss: 0.1967 - val_accuracy: 0.9486\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1149 - accuracy: 0.9648 - val_loss: 0.2071 - val_accuracy: 0.9502\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1050 - accuracy: 0.9678 - val_loss: 0.1972 - val_accuracy: 0.9472\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0927 - accuracy: 0.9711 - val_loss: 0.1884 - val_accuracy: 0.9496\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0854 - accuracy: 0.9738 - val_loss: 0.1890 - val_accuracy: 0.9558\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0771 - accuracy: 0.9763 - val_loss: 0.1884 - val_accuracy: 0.9534\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0711 - accuracy: 0.9782 - val_loss: 0.2066 - val_accuracy: 0.9526\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0632 - accuracy: 0.9810 - val_loss: 0.2051 - val_accuracy: 0.9548\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0579 - accuracy: 0.9817 - val_loss: 0.1999 - val_accuracy: 0.9574\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0540 - accuracy: 0.9830 - val_loss: 0.1970 - val_accuracy: 0.9570\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0476 - accuracy: 0.9850 - val_loss: 0.2160 - val_accuracy: 0.9548\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0435 - accuracy: 0.9867 - val_loss: 0.2417 - val_accuracy: 0.9524\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0402 - accuracy: 0.9877 - val_loss: 0.2252 - val_accuracy: 0.9566\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0381 - accuracy: 0.9877 - val_loss: 0.2234 - val_accuracy: 0.9576\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.2252 - val_accuracy: 0.9550\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0315 - accuracy: 0.9898 - val_loss: 0.2353 - val_accuracy: 0.9552\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0305 - accuracy: 0.9905 - val_loss: 0.2384 - val_accuracy: 0.9578\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0281 - accuracy: 0.9916 - val_loss: 0.2330 - val_accuracy: 0.9576\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0251 - accuracy: 0.9923 - val_loss: 0.2455 - val_accuracy: 0.9582\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0237 - accuracy: 0.9925 - val_loss: 0.2331 - val_accuracy: 0.9596\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0220 - accuracy: 0.9930 - val_loss: 0.2747 - val_accuracy: 0.9538\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0229 - accuracy: 0.9928 - val_loss: 0.2588 - val_accuracy: 0.9594\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0211 - accuracy: 0.9931 - val_loss: 0.2733 - val_accuracy: 0.9584\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0191 - accuracy: 0.9940 - val_loss: 0.2620 - val_accuracy: 0.9576\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.2815 - val_accuracy: 0.9592\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0186 - accuracy: 0.9936 - val_loss: 0.2741 - val_accuracy: 0.9594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7bfa67b190>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9534\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.18842054903507233, 0.9534000158309937]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 34s 16ms/step - loss: 0.4945 - accuracy: 0.8464 - val_loss: 0.2599 - val_accuracy: 0.9290\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.2805 - accuracy: 0.9145 - val_loss: 0.1857 - val_accuracy: 0.9510\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.2304 - accuracy: 0.9301 - val_loss: 0.1849 - val_accuracy: 0.9538\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.1989 - accuracy: 0.9400 - val_loss: 0.2112 - val_accuracy: 0.9504\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.1743 - accuracy: 0.9473 - val_loss: 0.1567 - val_accuracy: 0.9616\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.1597 - accuracy: 0.9523 - val_loss: 0.1554 - val_accuracy: 0.9652\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.1442 - accuracy: 0.9571 - val_loss: 0.1585 - val_accuracy: 0.9602\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 27s 15ms/step - loss: 0.1354 - accuracy: 0.9592 - val_loss: 0.1728 - val_accuracy: 0.9600\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.1240 - accuracy: 0.9625 - val_loss: 0.2139 - val_accuracy: 0.9622\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.1151 - accuracy: 0.9653 - val_loss: 0.1776 - val_accuracy: 0.9652\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.1070 - accuracy: 0.9673 - val_loss: 0.1891 - val_accuracy: 0.9674\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.1003 - accuracy: 0.9698 - val_loss: 0.2008 - val_accuracy: 0.9622\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.0921 - accuracy: 0.9718 - val_loss: 0.2522 - val_accuracy: 0.9666\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 27s 15ms/step - loss: 0.0886 - accuracy: 0.9728 - val_loss: 0.2228 - val_accuracy: 0.9678\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0867 - accuracy: 0.9733 - val_loss: 0.2394 - val_accuracy: 0.9632\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0817 - accuracy: 0.9750 - val_loss: 0.2565 - val_accuracy: 0.9650\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0763 - accuracy: 0.9778 - val_loss: 0.3001 - val_accuracy: 0.9668\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0757 - accuracy: 0.9779 - val_loss: 0.3939 - val_accuracy: 0.9630\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0702 - accuracy: 0.9794 - val_loss: 0.3256 - val_accuracy: 0.9660\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0661 - accuracy: 0.9802 - val_loss: 0.1954 - val_accuracy: 0.9672\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0650 - accuracy: 0.9809 - val_loss: 0.2809 - val_accuracy: 0.9692\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0622 - accuracy: 0.9814 - val_loss: 0.3025 - val_accuracy: 0.9670\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0594 - accuracy: 0.9821 - val_loss: 0.4671 - val_accuracy: 0.9628\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0593 - accuracy: 0.9819 - val_loss: 0.5836 - val_accuracy: 0.9694\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0551 - accuracy: 0.9832 - val_loss: 0.2699 - val_accuracy: 0.9676\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.0591 - accuracy: 0.9824 - val_loss: 0.4795 - val_accuracy: 0.9646\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 0.1554 - accuracy: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.15535463392734528, 0.9652000069618225]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now try adding Batch Normalization and compare the learning curves: is it\n",
    "# converging faster than before? Does it produce a better model?\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is the model overfitting the training set? Try adding dropout to every layer\n",
    "# and try again. Does it help?\n",
    "session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 14s 7ms/step - loss: 0.4407 - accuracy: 0.8745 - val_loss: 0.2230 - val_accuracy: 0.9440\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2337 - accuracy: 0.9409 - val_loss: 0.2081 - val_accuracy: 0.9482\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1934 - accuracy: 0.9526 - val_loss: 0.1966 - val_accuracy: 0.9540\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1694 - accuracy: 0.9592 - val_loss: 0.2199 - val_accuracy: 0.9506\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1589 - accuracy: 0.9626 - val_loss: 0.1503 - val_accuracy: 0.9662\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1457 - accuracy: 0.9648 - val_loss: 0.1934 - val_accuracy: 0.9534\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1414 - accuracy: 0.9657 - val_loss: 0.1696 - val_accuracy: 0.9646\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1343 - accuracy: 0.9688 - val_loss: 0.1575 - val_accuracy: 0.9620\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1328 - accuracy: 0.9688 - val_loss: 0.1488 - val_accuracy: 0.9632\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1203 - accuracy: 0.9712 - val_loss: 0.1933 - val_accuracy: 0.9658\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1303 - accuracy: 0.9676 - val_loss: 0.1477 - val_accuracy: 0.9716\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1216 - accuracy: 0.9701 - val_loss: 0.1515 - val_accuracy: 0.9674\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1135 - accuracy: 0.9735 - val_loss: 0.1417 - val_accuracy: 0.9672\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1435 - accuracy: 0.9683 - val_loss: 0.2637 - val_accuracy: 0.9316\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1162 - accuracy: 0.9713 - val_loss: 0.1163 - val_accuracy: 0.9764\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0960 - accuracy: 0.9761 - val_loss: 0.1207 - val_accuracy: 0.9744\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0923 - accuracy: 0.9781 - val_loss: 0.1402 - val_accuracy: 0.9670\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0926 - accuracy: 0.9775 - val_loss: 0.1489 - val_accuracy: 0.9648\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1008 - accuracy: 0.9745 - val_loss: 0.1703 - val_accuracy: 0.9616\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0767 - accuracy: 0.9813 - val_loss: 0.1285 - val_accuracy: 0.9738\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0996 - accuracy: 0.9756 - val_loss: 0.1652 - val_accuracy: 0.9648\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1101 - accuracy: 0.9723 - val_loss: 0.1562 - val_accuracy: 0.9634\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0979 - accuracy: 0.9751 - val_loss: 0.1265 - val_accuracy: 0.9712\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0912 - accuracy: 0.9768 - val_loss: 0.1791 - val_accuracy: 0.9632\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1354 - accuracy: 0.9635 - val_loss: 0.1659 - val_accuracy: 0.9556\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1710 - accuracy: 0.9603 - val_loss: 0.1444 - val_accuracy: 0.9678\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1085 - accuracy: 0.9727 - val_loss: 0.1296 - val_accuracy: 0.9706\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0834 - accuracy: 0.9789 - val_loss: 0.1276 - val_accuracy: 0.9692\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.0837 - accuracy: 0.9788 - val_loss: 0.1205 - val_accuracy: 0.9738\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.0700 - accuracy: 0.9828 - val_loss: 0.1361 - val_accuracy: 0.9764\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.0887 - accuracy: 0.9775 - val_loss: 0.2072 - val_accuracy: 0.9524\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.1049 - accuracy: 0.9737 - val_loss: 0.1270 - val_accuracy: 0.9718\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.0656 - accuracy: 0.9843 - val_loss: 0.1617 - val_accuracy: 0.9618\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1121 - accuracy: 0.9734 - val_loss: 0.1673 - val_accuracy: 0.9678\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1016 - accuracy: 0.9716 - val_loss: 0.2029 - val_accuracy: 0.9508\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.11630700528621674, 0.9764000177383423]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try replacing Batch Normalization with SELU, and make the necessary \n",
    "# adjustements to ensure the network self-normalizes \n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_selu_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5561 - accuracy: 0.8520 - val_loss: 0.2965 - val_accuracy: 0.9234\n",
      "Epoch 2/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2732 - accuracy: 0.9362 - val_loss: 0.1921 - val_accuracy: 0.9526\n",
      "Epoch 3/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2087 - accuracy: 0.9508 - val_loss: 0.1688 - val_accuracy: 0.9618\n",
      "Epoch 4/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1827 - accuracy: 0.9573 - val_loss: 0.1327 - val_accuracy: 0.9694\n",
      "Epoch 5/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1596 - accuracy: 0.9637 - val_loss: 0.1579 - val_accuracy: 0.9604\n",
      "Epoch 6/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1419 - accuracy: 0.9669 - val_loss: 0.1295 - val_accuracy: 0.9698\n",
      "Epoch 7/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1334 - accuracy: 0.9693 - val_loss: 0.1430 - val_accuracy: 0.9628\n",
      "Epoch 8/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1256 - accuracy: 0.9710 - val_loss: 0.1195 - val_accuracy: 0.9712\n",
      "Epoch 9/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1161 - accuracy: 0.9743 - val_loss: 0.1332 - val_accuracy: 0.9694\n",
      "Epoch 10/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1170 - accuracy: 0.9737 - val_loss: 0.1809 - val_accuracy: 0.9608\n",
      "Epoch 11/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1123 - accuracy: 0.9748 - val_loss: 0.1393 - val_accuracy: 0.9740\n",
      "Epoch 12/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0981 - accuracy: 0.9782 - val_loss: 0.1156 - val_accuracy: 0.9744\n",
      "Epoch 13/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.1060 - accuracy: 0.9766 - val_loss: 0.1295 - val_accuracy: 0.9716\n",
      "Epoch 14/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0910 - accuracy: 0.9798 - val_loss: 0.1567 - val_accuracy: 0.9656\n",
      "Epoch 15/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0923 - accuracy: 0.9794 - val_loss: 0.1299 - val_accuracy: 0.9724\n",
      "Epoch 16/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0865 - accuracy: 0.9811 - val_loss: 0.1346 - val_accuracy: 0.9700\n",
      "Epoch 17/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0815 - accuracy: 0.9821 - val_loss: 0.1290 - val_accuracy: 0.9738\n",
      "Epoch 18/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0804 - accuracy: 0.9824 - val_loss: 0.1167 - val_accuracy: 0.9738\n",
      "Epoch 19/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0779 - accuracy: 0.9824 - val_loss: 0.2027 - val_accuracy: 0.9548\n",
      "Epoch 20/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0800 - accuracy: 0.9805 - val_loss: 0.1329 - val_accuracy: 0.9742\n",
      "Epoch 21/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0938 - accuracy: 0.9808 - val_loss: 0.1270 - val_accuracy: 0.9746\n",
      "Epoch 22/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0869 - accuracy: 0.9798 - val_loss: 0.1110 - val_accuracy: 0.9768\n",
      "Epoch 23/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0700 - accuracy: 0.9835 - val_loss: 0.1261 - val_accuracy: 0.9702\n",
      "Epoch 24/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0776 - accuracy: 0.9844 - val_loss: 0.1140 - val_accuracy: 0.9738\n",
      "Epoch 25/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0552 - accuracy: 0.9875 - val_loss: 0.1426 - val_accuracy: 0.9702\n",
      "Epoch 26/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0653 - accuracy: 0.9853 - val_loss: 0.1168 - val_accuracy: 0.9760\n",
      "Epoch 27/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0508 - accuracy: 0.9889 - val_loss: 0.1236 - val_accuracy: 0.9738\n",
      "Epoch 28/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0610 - accuracy: 0.9858 - val_loss: 0.1283 - val_accuracy: 0.9746\n",
      "Epoch 29/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0578 - accuracy: 0.9875 - val_loss: 0.1189 - val_accuracy: 0.9782\n",
      "Epoch 30/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0635 - accuracy: 0.9862 - val_loss: 0.0998 - val_accuracy: 0.9804\n",
      "Epoch 31/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0758 - accuracy: 0.9847 - val_loss: 0.1310 - val_accuracy: 0.9678\n",
      "Epoch 32/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0597 - accuracy: 0.9862 - val_loss: 0.1155 - val_accuracy: 0.9786\n",
      "Epoch 33/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0447 - accuracy: 0.9899 - val_loss: 0.1107 - val_accuracy: 0.9800\n",
      "Epoch 34/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0516 - accuracy: 0.9887 - val_loss: 0.1141 - val_accuracy: 0.9782\n",
      "Epoch 35/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0627 - accuracy: 0.9854 - val_loss: 0.1221 - val_accuracy: 0.9772\n",
      "Epoch 36/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0498 - accuracy: 0.9887 - val_loss: 0.1550 - val_accuracy: 0.9738\n",
      "Epoch 37/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0537 - accuracy: 0.9883 - val_loss: 0.1837 - val_accuracy: 0.9720\n",
      "Epoch 38/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0642 - accuracy: 0.9850 - val_loss: 0.1300 - val_accuracy: 0.9762\n",
      "Epoch 39/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0460 - accuracy: 0.9901 - val_loss: 0.1251 - val_accuracy: 0.9770\n",
      "Epoch 40/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0371 - accuracy: 0.9916 - val_loss: 0.1307 - val_accuracy: 0.9776\n",
      "Epoch 41/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0546 - accuracy: 0.9874 - val_loss: 0.1308 - val_accuracy: 0.9740\n",
      "Epoch 42/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0482 - accuracy: 0.9893 - val_loss: 0.1247 - val_accuracy: 0.9766\n",
      "Epoch 43/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0452 - accuracy: 0.9900 - val_loss: 0.1495 - val_accuracy: 0.9784\n",
      "Epoch 44/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0364 - accuracy: 0.9913 - val_loss: 0.1616 - val_accuracy: 0.9662\n",
      "Epoch 45/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0488 - accuracy: 0.9886 - val_loss: 0.1974 - val_accuracy: 0.9776\n",
      "Epoch 46/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0340 - accuracy: 0.9925 - val_loss: 0.1503 - val_accuracy: 0.9788\n",
      "Epoch 47/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0432 - accuracy: 0.9898 - val_loss: 0.1822 - val_accuracy: 0.9670\n",
      "Epoch 48/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0430 - accuracy: 0.9901 - val_loss: 0.1507 - val_accuracy: 0.9806\n",
      "Epoch 49/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0385 - accuracy: 0.9918 - val_loss: 0.1335 - val_accuracy: 0.9784\n",
      "Epoch 50/100\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.0372 - accuracy: 0.9922 - val_loss: 0.1439 - val_accuracy: 0.9774\n",
      "157/157 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9804\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.09982352703809738, 0.980400025844574]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is the model overfitting the training set? Try adding dropout to every layer\n",
    "# and try again. Does it help?\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9804"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid)\n",
    "accuracy = np.mean(y_pred == y_valid)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,410\n",
      "Trainable params: 271,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preload_model = keras.models.load_model(\"my_model.h5\")\n",
    "\n",
    "# Take all layers except the last one\n",
    "new_model = keras.models.Sequential()\n",
    "for layer in preload_model.layers[:-1]:\n",
    "    new_model.add(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for layer in new_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new output layers\n",
    "new_model.add(keras.layers.Dense(10, activation=\"softmax\", name='main_output'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'callbacks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/github/ml_training/src/notebooks/training_dnn.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=1'>2</a>\u001b[0m early_stopping_cb \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(patience\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=3'>4</a>\u001b[0m new_model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=4'>5</a>\u001b[0m                 optimizer\u001b[39m=\u001b[39moptimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=5'>6</a>\u001b[0m                 metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=7'>8</a>\u001b[0m new_model\u001b[39m.\u001b[39mfit(X_train, y_train, epochs\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=8'>9</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39m(X_valid, y_valid),\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/github/ml_training/src/notebooks/training_dnn.ipynb#ch0000027?line=9'>10</a>\u001b[0m                 callbacks\u001b[39m=\u001b[39mcallbacks)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'callbacks' is not defined"
     ]
    }
   ],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_transfer_model.h5\", save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "new_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=optimizer,\n",
    "                metrics=[\"accuracy\"])\n",
    "\n",
    "new_model.fit(X_train, y_train, epochs=100,\n",
    "                validation_data=(X_valid, y_valid),\n",
    "                callbacks=callbacks)\n",
    "\n",
    "new_model = keras.models.load_model(\"my_transfer_model.h5\")\n",
    "new_model.evaluate(X_valid, y_valid)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e55b616c63168a6db6e336c7d9244a5e55794970d67bc6e4d9ec580cecf191"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('py386')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
