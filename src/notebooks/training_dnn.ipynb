{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises from _Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Is it okay to initialize all the weights to the same value as long as that value is\n",
    "selected randomly using He initialization?**\n",
    "\n",
    "Since we want to avoid symmetries by breaking them, initializing all values to the same will make \n",
    "all the weights to be the same, thus, making impossible to break the symmetry.\n",
    "\n",
    "**2. Is it okay to initialize the bias terms to 0?**\n",
    "\n",
    "Yes, it is okay, it doest not make much difference.\n",
    "\n",
    "**3. Name three advantages of the SELU activation function over ReLU.**\n",
    "\n",
    "It can take negative values, alliviating the vanishing gradients problem.\n",
    "It has a nonzero derivative, avoiding dying units.\n",
    "It is smooth everywhere, since ReLU jumps from 0 to 1 at given point.\n",
    "\n",
    "**4. In which cases would you want to use each of the following activation functions:\n",
    "ELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?**\n",
    "\n",
    "ELU, leaky ReLU: If you need the neural network to be as fast as possible.\n",
    "\n",
    "ReLU: Autoencoders.\n",
    "\n",
    "tanh: In a output layer if a number between -1 and 1 is needed.\n",
    "\n",
    "logistic: In the output layer to estimate a probability.\n",
    "\n",
    "softmax: In the output layer for probabilities that are mutually exclusive classes.\n",
    "\n",
    "**5. What may happen if you set the momentum hyperparameter too close to 1 (e.g.,\n",
    "0.99999) when using an SGD optimizer?**\n",
    "\n",
    "The algorithm will pick up a lot of speed and it will shoot right past the minimum. It will\n",
    "make it several times before converging, resulting in a slower training time.\n",
    "\n",
    "**6. Name three ways you can produce a sparse model.**\n",
    "\n",
    "Zero out tiny weights. Also apply l1 regularization during training, making it more sparse. At last combining\n",
    "l1 regularization with dual averaging.\n",
    "\n",
    "**7. Does dropout slow down training? Does it slow down inference (i.e., making\n",
    "predictions on new instances)? What are about MC dropout?**\n",
    "\n",
    "Yes, dropout does slow down training, in general roughly by a factor of two.\n",
    "However, it has no impact on inference since it is only turned on during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Adam optimization and early stopping, try training it on MNIST but\n",
    "# only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the\n",
    "# next exercise. You will need a softmax output layer with five neurons, and as\n",
    "# always make sure to save checkpoints at regular intervals and save the final\n",
    "# model so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layer\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=3e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping and checkpoint\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_model.h5\", save_best_only=True)\n",
    "\n",
    "run_index = 1 \n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 5.1812 - accuracy: 0.1368 - val_loss: 2.3657 - val_accuracy: 0.1692\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 2.2217 - accuracy: 0.1967 - val_loss: 2.1090 - val_accuracy: 0.2214\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 2.0644 - accuracy: 0.2411 - val_loss: 1.9905 - val_accuracy: 0.2658\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.9741 - accuracy: 0.2713 - val_loss: 1.9205 - val_accuracy: 0.2954\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.8969 - accuracy: 0.3040 - val_loss: 1.8321 - val_accuracy: 0.3304\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.8413 - accuracy: 0.3315 - val_loss: 1.8402 - val_accuracy: 0.3246\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.7969 - accuracy: 0.3440 - val_loss: 1.7837 - val_accuracy: 0.3506\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.7577 - accuracy: 0.3614 - val_loss: 1.7547 - val_accuracy: 0.3742\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.7231 - accuracy: 0.3758 - val_loss: 1.7257 - val_accuracy: 0.3808\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.6951 - accuracy: 0.3870 - val_loss: 1.6951 - val_accuracy: 0.3886\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.6651 - accuracy: 0.3965 - val_loss: 1.6904 - val_accuracy: 0.3882\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.6443 - accuracy: 0.4083 - val_loss: 1.6913 - val_accuracy: 0.3906\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.6197 - accuracy: 0.4159 - val_loss: 1.6673 - val_accuracy: 0.4048\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.6032 - accuracy: 0.4212 - val_loss: 1.6298 - val_accuracy: 0.4180\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.5869 - accuracy: 0.4292 - val_loss: 1.6401 - val_accuracy: 0.4160\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5651 - accuracy: 0.4361 - val_loss: 1.6218 - val_accuracy: 0.4150\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5514 - accuracy: 0.4423 - val_loss: 1.6049 - val_accuracy: 0.4208\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5349 - accuracy: 0.4482 - val_loss: 1.5969 - val_accuracy: 0.4298\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5228 - accuracy: 0.4546 - val_loss: 1.5990 - val_accuracy: 0.4224\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5114 - accuracy: 0.4581 - val_loss: 1.6048 - val_accuracy: 0.4230\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.4981 - accuracy: 0.4628 - val_loss: 1.6089 - val_accuracy: 0.4236\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4897 - accuracy: 0.4641 - val_loss: 1.5787 - val_accuracy: 0.4338\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4721 - accuracy: 0.4712 - val_loss: 1.6095 - val_accuracy: 0.4242\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4680 - accuracy: 0.4740 - val_loss: 1.5913 - val_accuracy: 0.4282\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4554 - accuracy: 0.4763 - val_loss: 1.5622 - val_accuracy: 0.4462\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4434 - accuracy: 0.4837 - val_loss: 1.5705 - val_accuracy: 0.4414\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4348 - accuracy: 0.4854 - val_loss: 1.5520 - val_accuracy: 0.4514\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4284 - accuracy: 0.4878 - val_loss: 1.5786 - val_accuracy: 0.4434\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4181 - accuracy: 0.4937 - val_loss: 1.5444 - val_accuracy: 0.4522\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4103 - accuracy: 0.4930 - val_loss: 1.5535 - val_accuracy: 0.4462\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3977 - accuracy: 0.4986 - val_loss: 1.5433 - val_accuracy: 0.4520\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3932 - accuracy: 0.5027 - val_loss: 1.5226 - val_accuracy: 0.4630\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 5s 4ms/step - loss: 1.3827 - accuracy: 0.5046 - val_loss: 1.5399 - val_accuracy: 0.4672\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3771 - accuracy: 0.5063 - val_loss: 1.5332 - val_accuracy: 0.4538\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3664 - accuracy: 0.5100 - val_loss: 1.5437 - val_accuracy: 0.4540\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3621 - accuracy: 0.5123 - val_loss: 1.5404 - val_accuracy: 0.4568\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3498 - accuracy: 0.5158 - val_loss: 1.5454 - val_accuracy: 0.4492\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3429 - accuracy: 0.5201 - val_loss: 1.5428 - val_accuracy: 0.4592\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3373 - accuracy: 0.5196 - val_loss: 1.5821 - val_accuracy: 0.4434\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.3258 - accuracy: 0.5251 - val_loss: 1.5444 - val_accuracy: 0.4622\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3226 - accuracy: 0.5260 - val_loss: 1.5576 - val_accuracy: 0.4506\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3158 - accuracy: 0.5290 - val_loss: 1.5492 - val_accuracy: 0.4608\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.3065 - accuracy: 0.5336 - val_loss: 1.5526 - val_accuracy: 0.4570\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3028 - accuracy: 0.5337 - val_loss: 1.5450 - val_accuracy: 0.4628\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2924 - accuracy: 0.5378 - val_loss: 1.5652 - val_accuracy: 0.4524\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2863 - accuracy: 0.5388 - val_loss: 1.5400 - val_accuracy: 0.4608\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2822 - accuracy: 0.5409 - val_loss: 1.5413 - val_accuracy: 0.4594\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2747 - accuracy: 0.5435 - val_loss: 1.5502 - val_accuracy: 0.4574\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.2662 - accuracy: 0.5481 - val_loss: 1.5536 - val_accuracy: 0.4618\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2606 - accuracy: 0.5499 - val_loss: 1.5592 - val_accuracy: 0.4542\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2585 - accuracy: 0.5488 - val_loss: 1.5587 - val_accuracy: 0.4486\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2485 - accuracy: 0.5529 - val_loss: 1.5749 - val_accuracy: 0.4486\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7d5c4fd550>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5226 - accuracy: 0.4630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5225569009780884, 0.46299999952316284]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 29s 16ms/step - loss: 1.8425 - accuracy: 0.3395 - val_loss: 1.6721 - val_accuracy: 0.4078\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.6712 - accuracy: 0.4025 - val_loss: 1.5720 - val_accuracy: 0.4352\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5979 - accuracy: 0.4318 - val_loss: 1.5345 - val_accuracy: 0.4482\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.5467 - accuracy: 0.4484 - val_loss: 1.4814 - val_accuracy: 0.4720\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.5025 - accuracy: 0.4669 - val_loss: 1.4358 - val_accuracy: 0.4880\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.4639 - accuracy: 0.4790 - val_loss: 1.4410 - val_accuracy: 0.4818\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.4328 - accuracy: 0.4916 - val_loss: 1.3999 - val_accuracy: 0.4982\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.4019 - accuracy: 0.5026 - val_loss: 1.3954 - val_accuracy: 0.5084\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 23s 17ms/step - loss: 1.3767 - accuracy: 0.5118 - val_loss: 1.3897 - val_accuracy: 0.5080\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3553 - accuracy: 0.5197 - val_loss: 1.3544 - val_accuracy: 0.5226\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.3393 - accuracy: 0.5240 - val_loss: 1.3558 - val_accuracy: 0.5172\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3172 - accuracy: 0.5360 - val_loss: 1.3851 - val_accuracy: 0.5166\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2963 - accuracy: 0.5404 - val_loss: 1.3599 - val_accuracy: 0.5144\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2790 - accuracy: 0.5462 - val_loss: 1.3497 - val_accuracy: 0.5326\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2603 - accuracy: 0.5553 - val_loss: 1.3673 - val_accuracy: 0.5268\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2509 - accuracy: 0.5558 - val_loss: 1.3684 - val_accuracy: 0.5218\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2295 - accuracy: 0.5638 - val_loss: 1.3270 - val_accuracy: 0.5318\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.2160 - accuracy: 0.5710 - val_loss: 1.3646 - val_accuracy: 0.5276\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1995 - accuracy: 0.5773 - val_loss: 1.3466 - val_accuracy: 0.5280\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1931 - accuracy: 0.5788 - val_loss: 1.3766 - val_accuracy: 0.5200\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1768 - accuracy: 0.5833 - val_loss: 1.3807 - val_accuracy: 0.5220\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.1591 - accuracy: 0.5898 - val_loss: 1.3296 - val_accuracy: 0.5272\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1527 - accuracy: 0.5944 - val_loss: 1.3253 - val_accuracy: 0.5426\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.1413 - accuracy: 0.5959 - val_loss: 1.3206 - val_accuracy: 0.5438\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1231 - accuracy: 0.6025 - val_loss: 1.3389 - val_accuracy: 0.5392\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.1123 - accuracy: 0.6075 - val_loss: 1.3574 - val_accuracy: 0.5354\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0986 - accuracy: 0.6129 - val_loss: 1.3394 - val_accuracy: 0.5388\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.0942 - accuracy: 0.6109 - val_loss: 1.3402 - val_accuracy: 0.5354\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0821 - accuracy: 0.6176 - val_loss: 1.3344 - val_accuracy: 0.5408\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0690 - accuracy: 0.6221 - val_loss: 1.3398 - val_accuracy: 0.5372\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0614 - accuracy: 0.6245 - val_loss: 1.3588 - val_accuracy: 0.5372\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0505 - accuracy: 0.6290 - val_loss: 1.3698 - val_accuracy: 0.5330\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0369 - accuracy: 0.6332 - val_loss: 1.3591 - val_accuracy: 0.5428\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0339 - accuracy: 0.6352 - val_loss: 1.3500 - val_accuracy: 0.5488\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0220 - accuracy: 0.6377 - val_loss: 1.3718 - val_accuracy: 0.5364\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0132 - accuracy: 0.6437 - val_loss: 1.3616 - val_accuracy: 0.5408\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9981 - accuracy: 0.6471 - val_loss: 1.3538 - val_accuracy: 0.5448\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9960 - accuracy: 0.6487 - val_loss: 1.3731 - val_accuracy: 0.5462\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9839 - accuracy: 0.6516 - val_loss: 1.3851 - val_accuracy: 0.5380\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9760 - accuracy: 0.6562 - val_loss: 1.3791 - val_accuracy: 0.5406\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9669 - accuracy: 0.6598 - val_loss: 1.3727 - val_accuracy: 0.5432\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9559 - accuracy: 0.6624 - val_loss: 1.3844 - val_accuracy: 0.5416\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9537 - accuracy: 0.6654 - val_loss: 1.3699 - val_accuracy: 0.5438\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 0.9400 - accuracy: 0.6671 - val_loss: 1.4311 - val_accuracy: 0.5286\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3206 - accuracy: 0.5438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3205863237380981, 0.5437999963760376]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now try adding Batch Normalization and compare the learning curves: is it\n",
    "# converging faster than before? Does it produce a better model?\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 12:58:37.931327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:58:37.931524: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:58:37.931674: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:58:37.931872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:58:37.931980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:58:37.932061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4708 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Is the model overfitting the training set? Try adding dropout to every layer\n",
    "# and try again. Does it help?\n",
    "session = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 12s 7ms/step - loss: 1.9415 - accuracy: 0.3041 - val_loss: 1.8389 - val_accuracy: 0.3416\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.7236 - accuracy: 0.3873 - val_loss: 1.6943 - val_accuracy: 0.3898\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.6223 - accuracy: 0.4295 - val_loss: 1.7451 - val_accuracy: 0.3762\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.5567 - accuracy: 0.4495 - val_loss: 1.6339 - val_accuracy: 0.4240\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.4967 - accuracy: 0.4749 - val_loss: 1.5786 - val_accuracy: 0.4428\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4483 - accuracy: 0.4936 - val_loss: 1.5303 - val_accuracy: 0.4580\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.4039 - accuracy: 0.5074 - val_loss: 1.5353 - val_accuracy: 0.4602\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3617 - accuracy: 0.5235 - val_loss: 1.4845 - val_accuracy: 0.4782\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.3261 - accuracy: 0.5340 - val_loss: 1.4812 - val_accuracy: 0.4776\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2943 - accuracy: 0.5469 - val_loss: 1.5210 - val_accuracy: 0.4892\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2681 - accuracy: 0.5606 - val_loss: 1.4911 - val_accuracy: 0.4974\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2383 - accuracy: 0.5682 - val_loss: 1.4879 - val_accuracy: 0.5074\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 1.2144 - accuracy: 0.5792 - val_loss: 1.4838 - val_accuracy: 0.4916\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.2349 - accuracy: 0.5678 - val_loss: 1.4881 - val_accuracy: 0.4862\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 1.1674 - accuracy: 0.5940 - val_loss: 1.4946 - val_accuracy: 0.4990\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.1332 - accuracy: 0.6076 - val_loss: 1.5073 - val_accuracy: 0.5040\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1138 - accuracy: 0.6136 - val_loss: 1.5599 - val_accuracy: 0.5086\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.1003 - accuracy: 0.6227 - val_loss: 1.5461 - val_accuracy: 0.4966\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 1.0919 - accuracy: 0.6245 - val_loss: 1.5754 - val_accuracy: 0.5038\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0737 - accuracy: 0.6314 - val_loss: 1.5584 - val_accuracy: 0.4988\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0558 - accuracy: 0.6388 - val_loss: 1.5273 - val_accuracy: 0.5090\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0357 - accuracy: 0.6448 - val_loss: 1.5711 - val_accuracy: 0.5060\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 1.0148 - accuracy: 0.6537 - val_loss: 1.5897 - val_accuracy: 0.4832\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9969 - accuracy: 0.6597 - val_loss: 1.6262 - val_accuracy: 0.5100\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9814 - accuracy: 0.6648 - val_loss: 1.5814 - val_accuracy: 0.5048\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9642 - accuracy: 0.6718 - val_loss: 1.5956 - val_accuracy: 0.5018\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9762 - accuracy: 0.6706 - val_loss: 1.5575 - val_accuracy: 0.5024\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 11s 8ms/step - loss: 0.9418 - accuracy: 0.6778 - val_loss: 1.5746 - val_accuracy: 0.4948\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.9190 - accuracy: 0.6886 - val_loss: 1.6753 - val_accuracy: 0.5030\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4812 - accuracy: 0.4776\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4811910390853882, 0.47760000824928284]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try replacing Batch Normalization with SELU, and make the necessary \n",
    "# adjustements to ensure the network self-normalizes \n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 6s 4ms/step - loss: 2.1066 - accuracy: 0.2699 - val_loss: 1.8124 - val_accuracy: 0.3394\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.7617 - accuracy: 0.3796 - val_loss: 1.6445 - val_accuracy: 0.4180\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.6327 - accuracy: 0.4271 - val_loss: 1.5905 - val_accuracy: 0.4272\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.5597 - accuracy: 0.4567 - val_loss: 1.5691 - val_accuracy: 0.4472\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4998 - accuracy: 0.4775 - val_loss: 1.5576 - val_accuracy: 0.4562\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4495 - accuracy: 0.4974 - val_loss: 1.4846 - val_accuracy: 0.4798\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.4111 - accuracy: 0.5136 - val_loss: 1.5518 - val_accuracy: 0.4636\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3748 - accuracy: 0.5268 - val_loss: 1.4838 - val_accuracy: 0.4854\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3353 - accuracy: 0.5382 - val_loss: 1.4887 - val_accuracy: 0.4812\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.3030 - accuracy: 0.5534 - val_loss: 1.5202 - val_accuracy: 0.4882\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.2710 - accuracy: 0.5668 - val_loss: 1.5005 - val_accuracy: 0.4912\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.2538 - accuracy: 0.5712 - val_loss: 1.4733 - val_accuracy: 0.4934\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.2203 - accuracy: 0.5842 - val_loss: 1.5008 - val_accuracy: 0.5024\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.1986 - accuracy: 0.5908 - val_loss: 1.4870 - val_accuracy: 0.4916\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.1720 - accuracy: 0.6030 - val_loss: 1.5285 - val_accuracy: 0.4972\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.1438 - accuracy: 0.6119 - val_loss: 1.5250 - val_accuracy: 0.5088\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.1255 - accuracy: 0.6187 - val_loss: 1.5063 - val_accuracy: 0.4996\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.1045 - accuracy: 0.6287 - val_loss: 1.5252 - val_accuracy: 0.5006\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.0850 - accuracy: 0.6338 - val_loss: 1.5451 - val_accuracy: 0.5128\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.0673 - accuracy: 0.6434 - val_loss: 1.5133 - val_accuracy: 0.5030\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.0482 - accuracy: 0.6491 - val_loss: 1.5678 - val_accuracy: 0.5100\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.0226 - accuracy: 0.6548 - val_loss: 1.6031 - val_accuracy: 0.5074\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 1.0064 - accuracy: 0.6654 - val_loss: 1.6114 - val_accuracy: 0.4886\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.9932 - accuracy: 0.6684 - val_loss: 1.5940 - val_accuracy: 0.5054\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.0162 - accuracy: 0.6669 - val_loss: 1.6441 - val_accuracy: 0.4532\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 1.0734 - accuracy: 0.6371 - val_loss: 1.6069 - val_accuracy: 0.4784\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.9523 - accuracy: 0.6832 - val_loss: 1.5910 - val_accuracy: 0.5044\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.9213 - accuracy: 0.6941 - val_loss: 1.5801 - val_accuracy: 0.5024\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 4s 3ms/step - loss: 0.9159 - accuracy: 0.6971 - val_loss: 1.6465 - val_accuracy: 0.5152\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.9055 - accuracy: 0.7030 - val_loss: 1.6173 - val_accuracy: 0.5048\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.8972 - accuracy: 0.7031 - val_loss: 1.6172 - val_accuracy: 0.5114\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 5s 3ms/step - loss: 0.8809 - accuracy: 0.7088 - val_loss: 1.7906 - val_accuracy: 0.4878\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.4733 - accuracy: 0.4934\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.473311185836792, 0.4934000074863434]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Is the model overfitting the training set? Try adding dropout to every layer\n",
    "# and try again. Does it help?\n",
    "\n",
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.Dropout(rate=0.5))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4982"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "90e55b616c63168a6db6e336c7d9244a5e55794970d67bc6e4d9ec580cecf191"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('py386')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
